{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68a27a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-29T15:34:05.464565Z",
     "iopub.status.busy": "2025-07-29T15:34:05.464359Z",
     "iopub.status.idle": "2025-07-29T15:34:07.245034Z",
     "shell.execute_reply": "2025-07-29T15:34:07.244081Z"
    },
    "papermill": {
     "duration": 1.785282,
     "end_time": "2025-07-29T15:34:07.246481",
     "exception": false,
     "start_time": "2025-07-29T15:34:05.461199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/model.safetensors.index.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/model-00003-of-00003.safetensors\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/config.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/preprocessor_config.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/README.md\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/tokenizer.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/model-00001-of-00003.safetensors\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/tokenizer_config.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/chat_template.jinja\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/model-00002-of-00003.safetensors\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/processor_config.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/special_tokens_map.json\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/tokenizer.model\n",
      "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1/generation_config.json\n",
      "/kaggle/input/google-gemma-3n-hackathon/README\n",
      "/kaggle/input/google-gemma-3n-hackathon/archive/README\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386c9e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T15:34:07.252385Z",
     "iopub.status.busy": "2025-07-29T15:34:07.252086Z",
     "iopub.status.idle": "2025-07-29T15:39:33.239106Z",
     "shell.execute_reply": "2025-07-29T15:39:33.238212Z"
    },
    "papermill": {
     "duration": 326.017948,
     "end_time": "2025-07-29T15:39:33.266749",
     "exception": false,
     "start_time": "2025-07-29T15:34:07.248801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 15:36:29.513622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753803389.854234      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753803389.950381      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 14.7 GB\n",
      "Authenticating and downloading Gemma 3n model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f5f6600b3d424f8363b3af5874813a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gemma_e2b_it...\n",
      "✓ gemma_e2b_it downloaded to: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n",
      "Model ready at: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n",
      "Loading model and processor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710763c8c0d04e3d926757a7fc6405c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma3nForConditionalGeneration were not initialized from the model checkpoint at /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1 and are newly initialized: ['model.vision_tower.timm_model.conv_stem.conv.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded on cuda\n",
      "✓ Model parameters: ~5.4B\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Agritech Advisor Demo with Gemma 3n (E2B-IT) - Enhanced Version\n",
    "# ==========================\n",
    "\"\"\"\n",
    "Enhanced Agritech Advisor powered by Gemma 3n multimodal AI model.\n",
    "\n",
    "This notebook demonstrates a comprehensive agricultural advisory system that:\n",
    "- Processes multimodal inputs (text, image, audio)\n",
    "- Provides AI-powered crop health analysis\n",
    "- Offers actionable agricultural recommendations\n",
    "- Supports up to 32K input/output tokens\n",
    "- Works offline once models are downloaded\n",
    "\n",
    "Author: DataZimbo\n",
    "Date: 2025-07-29\n",
    "\"\"\"\n",
    "\n",
    "# --------\n",
    "# 1. Install Dependencies\n",
    "# --------\n",
    "# Install required packages for multimodal AI processing\n",
    "# - timm: PyTorch Image Models for advanced computer vision\n",
    "# - accelerate: Hugging Face library for distributed/mixed precision training\n",
    "# - transformers: State-of-the-art NLP and multimodal models\n",
    "# - kagglehub: Official Kaggle API for downloading datasets and models\n",
    "!pip install -q --upgrade timm accelerate git+https://github.com/huggingface/transformers.git kagglehub\n",
    "\n",
    "# --------\n",
    "# 2. Imports and Setup\n",
    "# --------\n",
    "import os              # Operating system interface for environment variables\n",
    "import gc              # Garbage collector for memory management\n",
    "import re              # Regular expressions for text processing\n",
    "import json            # JSON handling for structured data\n",
    "import warnings        # Warning control for cleaner output\n",
    "import torch           # PyTorch deep learning framework\n",
    "from PIL import Image  # Python Imaging Library for image processing\n",
    "from io import BytesIO # In-memory binary streams for image handling\n",
    "from IPython.display import display, HTML  # Jupyter notebook display utilities\n",
    "import base64          # Base64 encoding for image display\n",
    "import requests        # HTTP requests for fetching images from URLs\n",
    "import kagglehub       # Kaggle Hub API for model downloading\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor  # Hugging Face transformers\n",
    "\n",
    "# --------\n",
    "# 3. Environment and Device Configuration\n",
    "# --------\n",
    "# Suppress unnecessary warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Disable PyTorch optimizations that might cause issues in notebook environments\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"  # Disable TorchInductor compilation\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"1\"   # Disable torch.compile optimization\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available compute device.\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: CUDA GPU if available, otherwise CPU\n",
    "        \n",
    "    Note:\n",
    "        GPU acceleration significantly improves inference speed for large models\n",
    "    \"\"\"\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize device and display current configuration\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "\n",
    "# --------\n",
    "# 4. KaggleHub Authentication and Model Download (Gemma 3n E2B-IT)\n",
    "# --------\n",
    "# Define Gemma 3n model resource configuration\n",
    "# E2B-IT stands for \"End-to-End Bilingual Instruction Tuned\"\n",
    "GEMMA_MODEL_RESOURCE = {\n",
    "    \"gemma_e2b_it\": {\n",
    "        \"model_download\": \"google/gemma-3n/Transformers/gemma-3n-e2b-it/1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def authenticate_and_download_kaggle_resources(resource_dict):\n",
    "    \"\"\"\n",
    "    Authenticate with Kaggle and download specified resources.\n",
    "    \n",
    "    Args:\n",
    "        resource_dict (dict): Dictionary containing resource configurations\n",
    "                             with 'competition_download' or 'model_download' keys\n",
    "    \n",
    "    Returns:\n",
    "        dict: Local file paths for downloaded resources\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If unknown resource type is encountered\n",
    "        \n",
    "    Note:\n",
    "        Requires Kaggle API credentials to be configured\n",
    "        (kaggle.json in ~/.kaggle/ or environment variables)\n",
    "    \"\"\"\n",
    "    # Authenticate with Kaggle Hub using stored credentials\n",
    "    kagglehub.login()\n",
    "    \n",
    "    local_paths = {}\n",
    "    for name, resource in resource_dict.items():\n",
    "        print(f\"Downloading {name}...\")\n",
    "        \n",
    "        if 'competition_download' in resource:\n",
    "            # Download competition dataset\n",
    "            local_paths[name] = kagglehub.competition_download(resource['competition_download'])\n",
    "        elif 'model_download' in resource:\n",
    "            # Download pre-trained model\n",
    "            local_paths[name] = kagglehub.model_download(resource['model_download'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resource type for {name}: {resource}\")\n",
    "            \n",
    "        print(f\"✓ {name} downloaded to: {local_paths[name]}\")\n",
    "    \n",
    "    return local_paths\n",
    "\n",
    "# Download Gemma 3n model and get local path\n",
    "print(\"Authenticating and downloading Gemma 3n model...\")\n",
    "RESOURCES = authenticate_and_download_kaggle_resources(GEMMA_MODEL_RESOURCE)\n",
    "GEMMA_E2B_IT_PATH = RESOURCES[\"gemma_e2b_it\"]\n",
    "print(f\"Model ready at: {GEMMA_E2B_IT_PATH}\")\n",
    "\n",
    "# --------\n",
    "# 5. Model and Processor Loading\n",
    "# --------\n",
    "def load_hf_model_and_processor(model_path, model_class=AutoModelForImageTextToText, \n",
    "                               processor_class=AutoProcessor, device=None):\n",
    "    \"\"\"\n",
    "    Load Hugging Face model and processor from local path.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Local path to the downloaded model\n",
    "        model_class: Hugging Face model class (default: AutoModelForImageTextToText)\n",
    "        processor_class: Hugging Face processor class (default: AutoProcessor)\n",
    "        device: Target device for model inference\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, processor) ready for inference\n",
    "        \n",
    "    Note:\n",
    "        - Uses automatic mixed precision (torch_dtype=\"auto\") for efficiency\n",
    "        - Loads model to specified device for faster inference\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    print(\"Loading model and processor...\")\n",
    "    \n",
    "    # Load the multimodal model with automatic precision\n",
    "    model = model_class.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=None,           # Manual device placement\n",
    "        torch_dtype=\"auto\"         # Automatic precision selection\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the corresponding processor for input preprocessing\n",
    "    processor = processor_class.from_pretrained(model_path)\n",
    "    \n",
    "    print(f\"✓ Model loaded on {device}\")\n",
    "    print(f\"✓ Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Load Gemma 3n model and processor\n",
    "model, processor = load_hf_model_and_processor(GEMMA_E2B_IT_PATH, device=device)\n",
    "\n",
    "# --------\n",
    "# 6. Input Utilities (Text, Image, Audio)\n",
    "# --------\n",
    "def detect_input_type(input_data):\n",
    "    \"\"\"\n",
    "    Automatically detect input type based on data characteristics.\n",
    "    \n",
    "    Args:\n",
    "        input_data (str): Input data (file path, URL, or text)\n",
    "    \n",
    "    Returns:\n",
    "        str: \"image\", \"audio\", or \"text\"\n",
    "        \n",
    "    Examples:\n",
    "        >>> detect_input_type(\"photo.jpg\")\n",
    "        \"image\"\n",
    "        >>> detect_input_type(\"recording.wav\")\n",
    "        \"audio\"\n",
    "        >>> detect_input_type(\"My crops are wilting\")\n",
    "        \"text\"\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, str):\n",
    "        # Check for image file extensions\n",
    "        if input_data.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n",
    "            return \"image\"\n",
    "        # Check for audio file extensions\n",
    "        elif input_data.lower().endswith((\".wav\", \".mp3\", \".m4a\", \".ogg\", \".flac\")):\n",
    "            return \"audio\"\n",
    "    \n",
    "    # Default to text for everything else\n",
    "    return \"text\"\n",
    "\n",
    "def fetch_image(raw_input):\n",
    "    \"\"\"\n",
    "    Fetch and load image from URL or local file path.\n",
    "    \n",
    "    Args:\n",
    "        raw_input (str): Image URL or local file path\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: RGB image ready for processing\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If image cannot be loaded or accessed\n",
    "        \n",
    "    Note:\n",
    "        Automatically converts images to RGB format for consistency\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if raw_input.startswith(\"http://\") or raw_input.startswith(\"https://\"):\n",
    "            # Fetch image from URL\n",
    "            print(f\"Fetching image from URL: {raw_input}\")\n",
    "            response = requests.get(raw_input, timeout=30)\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            # Load image from local file\n",
    "            print(f\"Loading image from file: {raw_input}\")\n",
    "            image = Image.open(raw_input).convert(\"RGB\")\n",
    "        \n",
    "        print(f\"✓ Image loaded: {image.size[0]}x{image.size[1]} pixels\")\n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load image from {raw_input}: {e}\")\n",
    "\n",
    "def normalize_image(image, target_size=512):\n",
    "    \"\"\"\n",
    "    Normalize image to supported dimensions for Gemma 3n processing.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image to normalize\n",
    "        target_size (int): Desired output size (default: 512)\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: Resized image with dimensions from [256, 512, 768]\n",
    "        \n",
    "    Note:\n",
    "        Gemma 3n supports specific image sizes for optimal performance.\n",
    "        The function selects the closest supported size to the target.\n",
    "    \"\"\"\n",
    "    # Gemma 3n supported image dimensions\n",
    "    allowed_sizes = [256, 512, 768]\n",
    "    \n",
    "    # Select closest allowed size to target\n",
    "    if isinstance(target_size, int):\n",
    "        size = min(allowed_sizes, key=lambda x: abs(x - target_size))\n",
    "    else:\n",
    "        size = 512  # Default fallback\n",
    "    \n",
    "    print(f\"Normalizing image to {size}x{size} pixels\")\n",
    "    normalized_image = image.resize((size, size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def display_resized_image(image, width_px=300):\n",
    "    \"\"\"\n",
    "    Display PIL image in Jupyter notebook with specified width.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Image to display\n",
    "        width_px (int): Display width in pixels (default: 300)\n",
    "        \n",
    "    Note:\n",
    "        Converts image to base64 for HTML embedding in notebook\n",
    "    \"\"\"\n",
    "    # Convert PIL image to base64 for HTML display\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_b64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    \n",
    "    # Create HTML img tag with base64 data\n",
    "    html = f'<img src=\"data:image/png;base64,{img_b64}\" width=\"{width_px}px\" style=\"border-radius: 8px;\"/>'\n",
    "    display(HTML(html))\n",
    "\n",
    "def format_input(input_type, raw_input):\n",
    "    \"\"\"\n",
    "    Process and format input data based on detected type.\n",
    "    \n",
    "    Args:\n",
    "        input_type (str): Type of input (\"image\", \"audio\", or \"text\")\n",
    "        raw_input: Raw input data (URL, file path, or text string)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted input ready for model processing\n",
    "        \n",
    "    Note:\n",
    "        - Images are normalized to supported dimensions\n",
    "        - Audio is converted to text via simulated ASR (placeholder)\n",
    "        - Text is passed through unchanged\n",
    "    \"\"\"\n",
    "    if input_type == \"image\":\n",
    "        # Process image input\n",
    "        image = fetch_image(raw_input)\n",
    "        image = normalize_image(image, target_size=512)\n",
    "        \n",
    "        print(\"📸 Image processed successfully. Preview:\")\n",
    "        display_resized_image(image, width_px=300)\n",
    "        return image\n",
    "        \n",
    "    elif input_type == \"audio\":\n",
    "        # Simulate audio-to-text conversion\n",
    "        # In production, this would use a speech recognition model\n",
    "        simulated_transcript = \"Leaves turning yellow and curling at the edges.\"\n",
    "        print(f\"🎤 Audio input detected (simulated)\")\n",
    "        print(f\"   Transcript: '{simulated_transcript}'\")\n",
    "        print(\"   Note: In production, this would use actual ASR processing\")\n",
    "        return simulated_transcript\n",
    "        \n",
    "    elif input_type == \"text\":\n",
    "        # Text input passes through unchanged\n",
    "        print(f\"📝 Text input: '{raw_input[:100]}{'...' if len(raw_input) > 100 else ''}'\")\n",
    "        return raw_input\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input type: {input_type}\")\n",
    "\n",
    "# --------\n",
    "# 7. Agritech Instructions and Prompt Engineering\n",
    "# --------\n",
    "AGRITECH_ADVISOR_INSTRUCTIONS = \"\"\"\n",
    "You are an expert Agritech Advisor powered by Gemma 3n multimodal AI.\n",
    "\n",
    "Your role is to analyze agricultural inputs and provide actionable advice to farmers.\n",
    "You can process images of crops, transcribed audio questions, or written text queries.\n",
    "\n",
    "ANALYSIS WORKFLOW:\n",
    "1. **Input Detection**: Identify if input is visual (crop image) or textual (question/description)\n",
    "\n",
    "2. **Image Analysis** (if applicable):\n",
    "   - Examine crop health indicators (leaf color, texture, growth patterns)\n",
    "   - Identify potential diseases, pests, or nutrient deficiencies\n",
    "   - Assess environmental stress signs (drought, overwatering, etc.)\n",
    "   - Note growth stage and overall plant condition\n",
    "\n",
    "3. **Problem Diagnosis**:\n",
    "   - Classify issues: disease, pest, nutrient deficiency, environmental stress\n",
    "   - Determine severity level and urgency of intervention\n",
    "   - Consider regional context if provided (climate, season, location)\n",
    "\n",
    "4. **Recommendation Generation**:\n",
    "   - Provide specific, actionable solutions\n",
    "   - Include both immediate and long-term strategies\n",
    "   - Consider resource availability and farmer accessibility\n",
    "   - Prioritize sustainable and cost-effective approaches\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Always respond with properly formatted JSON enclosed in triple backticks:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input_type\": \"<image/speech/text>\",\n",
    "  \"analysis\": \"<detailed observation of input or visual analysis>\",\n",
    "  \"diagnosis\": \"<specific issue identified or 'healthy' if no problems>\",\n",
    "  \"confidence\": \"<high/medium/low based on analysis certainty>\",\n",
    "  \"advice\": \"<primary actionable recommendation>\",\n",
    "  \"urgency\": \"<low/medium/high based on problem severity>\",\n",
    "  \"tips\": [\"<specific tip 1>\", \"<specific tip 2>\", \"<specific tip 3>\"],\n",
    "  \"timeline\": \"<immediate/within days/within weeks - when to act>\",\n",
    "  \"prevention\": \"<how to prevent this issue in future>\"\n",
    "} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a450777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T15:39:33.319451Z",
     "iopub.status.busy": "2025-07-29T15:39:33.319199Z",
     "iopub.status.idle": "2025-07-29T15:44:08.521089Z",
     "shell.execute_reply": "2025-07-29T15:44:08.520264Z"
    },
    "papermill": {
     "duration": 275.255551,
     "end_time": "2025-07-29T15:44:08.547838",
     "exception": false,
     "start_time": "2025-07-29T15:39:33.292287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 Starting Agritech Advisor Analysis...\n",
      "==================================================\n",
      "📋 Input type detected: TEXT\n",
      "📝 Text input: 'My tomato plants have yellow leaves with brown spots. What should I do?'\n",
      "\n",
      "🔄 Processing with Gemma 3n...\n",
      "⚡ Generating AI response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 15:40:33.382000 19 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: generator\n"
     ]
    }
   ],
   "source": [
    "def run_agritech_advisor(input_data, context_hint=\"\", model=None, processor=None, device=None, max_tokens=2048):\n",
    "    \"\"\"Main function to run the Agritech Advisor on various input types.\"\"\"\n",
    "    print(\"🌱 Starting Agritech Advisor Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    if model is None or processor is None:\n",
    "        raise ValueError(\"Model and processor must be provided\")\n",
    "    \n",
    "    try:\n",
    "        # Detect and process input type\n",
    "        input_type = detect_input_type(input_data)\n",
    "        print(f\"📋 Input type detected: {input_type.upper()}\")\n",
    "        \n",
    "        # Format input data\n",
    "        formatted_input = format_input(input_type, input_data)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"{AGRITECH_ADVISOR_INSTRUCTIONS}\n",
    "CONTEXT: {context_hint}\n",
    "ANALYZE THE FOLLOWING INPUT:\n",
    "\"\"\"\n",
    "        \n",
    "        # Prepare inputs for model\n",
    "        print(\"\\n🔄 Processing with Gemma 3n...\")\n",
    "        \n",
    "        if input_type == \"image\":\n",
    "            inputs = processor(text=prompt, images=formatted_input, return_tensors=\"pt\").to(device)\n",
    "        else:\n",
    "            full_text = prompt + str(formatted_input)\n",
    "            inputs = processor(text=full_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        print(\"⚡ Generating AI response...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"✅ Response generated!\")\n",
    "        print(\"\\nRaw response:\")\n",
    "        print(response)\n",
    "        \n",
    "        return {\"response\": response}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Now you can run your example\n",
    "result = run_agritech_advisor(\n",
    "    input_data=\"My tomato plants have yellow leaves with brown spots. What should I do?\",\n",
    "    context_hint=\"Tomatoes, Kenya, dry season\",\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386e9c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T15:44:08.646313Z",
     "iopub.status.busy": "2025-07-29T15:44:08.646025Z",
     "iopub.status.idle": "2025-07-29T15:44:10.324626Z",
     "shell.execute_reply": "2025-07-29T15:44:10.323840Z"
    },
    "papermill": {
     "duration": 1.752283,
     "end_time": "2025-07-29T15:44:10.326034",
     "exception": false,
     "start_time": "2025-07-29T15:44:08.573751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 Starting Agritech Advisor Analysis...\n",
      "==================================================\n",
      "📋 Input type detected: TEXT\n",
      "📝 Text input: 'My tomato plants have yellow leaves with brown spots. What should I do?'\n",
      "\n",
      "🔄 Processing with Gemma 3n...\n",
      "⚡ Generating AI response...\n",
      "❌ Error: generator\n"
     ]
    }
   ],
   "source": [
    "# Run a text-based agricultural question\n",
    "result = run_agritech_advisor(\n",
    "    input_data=\"My tomato plants have yellow leaves with brown spots. What should I do?\",\n",
    "    context_hint=\"Tomatoes, Kenya, dry season\",\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12693789,
     "sourceId": 105267,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 12845828,
     "modelInstanceId": 365533,
     "sourceId": 450403,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 613.144265,
   "end_time": "2025-07-29T15:44:13.273133",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-29T15:34:00.128868",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0284aee5e8b748cc99b6ee97586948a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0c7836169b7e45349593335c00947322": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "PasswordModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "PasswordModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "PasswordView",
       "continuous_update": true,
       "description": "Token:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_8826dc63d80343f3986f523b92f28b14",
       "placeholder": "​",
       "style": "IPY_MODEL_0284aee5e8b748cc99b6ee97586948a1",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "12eba7665901487d8e62ad613132e1fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1792231f65e642bf86aacd6123f06342": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1dce56ee3e924b00a60e0964c055ba06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e0bc554822b48e68d3fb94e5943cf1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1fe451cd7c39463facb6be0afbba9fb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Username:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_d37dba0e8a4540d0b5eb523b0c9dd749",
       "placeholder": "​",
       "style": "IPY_MODEL_1792231f65e642bf86aacd6123f06342",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "2220a50765fa4a3fbbcc7a138bcce477": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "35e1cbe9fa224bcbb1fabef8a4b962dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3e940e23a28f4b41a2f39ea19a9122f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ButtonModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ButtonView",
       "button_style": "",
       "description": "Login",
       "disabled": false,
       "icon": "",
       "layout": "IPY_MODEL_12eba7665901487d8e62ad613132e1fa",
       "style": "IPY_MODEL_d32b514e1f3d43ea8ef504932e0f209c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3f419908e2fd493f87100d1c34e2f8e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6df37ab288f14428a87c55a1c690286b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6f111df090ad4209addc7e789c6d8610": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "710763c8c0d04e3d926757a7fc6405c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_854e27872d214a63b2fe3e6b3dd515c2",
        "IPY_MODEL_88756ebcef984db588ea96bbacb74f5b",
        "IPY_MODEL_a09dc091015a4a46bf9685de8ed9dfe4"
       ],
       "layout": "IPY_MODEL_6f111df090ad4209addc7e789c6d8610",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7d24bca2b8be4a8487afcc8327bec6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "854e27872d214a63b2fe3e6b3dd515c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1dce56ee3e924b00a60e0964c055ba06",
       "placeholder": "​",
       "style": "IPY_MODEL_2220a50765fa4a3fbbcc7a138bcce477",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "8826dc63d80343f3986f523b92f28b14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88756ebcef984db588ea96bbacb74f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a795d955ed514f7ab5799dabb0327606",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35e1cbe9fa224bcbb1fabef8a4b962dd",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "90c80cab8e4e45238c36d0d38603d0d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a09dc091015a4a46bf9685de8ed9dfe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3f419908e2fd493f87100d1c34e2f8e2",
       "placeholder": "​",
       "style": "IPY_MODEL_1e0bc554822b48e68d3fb94e5943cf1d",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:06&lt;00:00,  1.89s/it]"
      }
     },
     "a795d955ed514f7ab5799dabb0327606": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c27a3a2dcaf94da6b4b305baaffa086d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ddee322cce264dfa9969c64cc7107a96",
       "placeholder": "​",
       "style": "IPY_MODEL_6df37ab288f14428a87c55a1c690286b",
       "tabbable": null,
       "tooltip": null,
       "value": "\n<b>Thank You</b></center>"
      }
     },
     "cf40ebacd46b40478b6dd00dbc1539b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": "center",
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "flex",
       "flex": null,
       "flex_flow": "column",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "50%"
      }
     },
     "d32b514e1f3d43ea8ef504932e0f209c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ButtonStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "button_color": null,
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "d37dba0e8a4540d0b5eb523b0c9dd749": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9f5f6600b3d424f8363b3af5874813a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fbfb05c6bfd04d498375364fd50cde6b",
        "IPY_MODEL_1fe451cd7c39463facb6be0afbba9fb1",
        "IPY_MODEL_0c7836169b7e45349593335c00947322",
        "IPY_MODEL_3e940e23a28f4b41a2f39ea19a9122f8",
        "IPY_MODEL_c27a3a2dcaf94da6b4b305baaffa086d"
       ],
       "layout": "IPY_MODEL_cf40ebacd46b40478b6dd00dbc1539b8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ddee322cce264dfa9969c64cc7107a96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbfb05c6bfd04d498375364fd50cde6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_90c80cab8e4e45238c36d0d38603d0d3",
       "placeholder": "​",
       "style": "IPY_MODEL_7d24bca2b8be4a8487afcc8327bec6bc",
       "tabbable": null,
       "tooltip": null,
       "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
